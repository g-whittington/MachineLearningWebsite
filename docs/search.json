[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am George Whittington\nAs an undergraduate student majoring in statistics, Iâ€™m particularly drawn to the exciting field of machine learning. I love the process of building models and seeing how they can solve real-world problems.\nI actively use both R and Python in my work, and Iâ€™m committed to transparency and collaboration by making my projects available on public repositories.\nIf I could describe myself using three emojis I would select: ğŸ‘¾ ğŸ˜¼ ğŸ˜"
  },
  {
    "objectID": "models/datasets.html",
    "href": "models/datasets.html",
    "title": "Data and Research Question",
    "section": "",
    "text": "The Youth Risk Behavior Survey (YRBS) is a national survey that monitors health-related behaviors among high school students, including weapon carrying and associated risk factors.\n\n\n\nSource: Centers for Disease Control and Prevention (CDC)\nYear: 2023\nTarget Population: High school students\nSample Size: Approximately 19,000 students nationwide\n\n\n\n\nHow do logistic regression, lasso, k-nearest neighbors, and tree-based models compare in predicting school-based weapon carrying among adolescents based on risk and protective factors?\n\n\n\nThe dataset includes information on various health-related behaviors:\n\nOutcome\n\nWeapon Carrying (Carried a weapon on school property)\n\nPredictors\n\nTraumatic experiences\nSchool Safety Perceptions\nBullying Experiences\nFamily Support\nSocial Media Use\nPeer Relationships\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(dissertationData)\nlibrary(here)\ndata(clean_yrbs_2023)\n# Add your data preprocessing code here\n\n\n\n\nWe will do it in classâ€¦\n\n\n\n\n# This is an example of how to create a dataset for a model.\n# You can use this as a template to create your own dataset.\n\n\nanalysis_data &lt;- clean_yrbs_2023 %&gt;%\n    select(\n        WeaponCarryingSchool, AttackedInNeighborhood, Bullying,\n        SexualAbuseByOlderPerson, ParentalPhysicalAbuse, ParentSubstanceUse,\n        ParentIncarceration, SchoolConnectedness, ParentalMonitoring,\n        UnfairDisciplineAtSchool, Homelessness\n    ) |&gt;\n    filter(!is.na(WeaponCarryingSchool)) %&gt;%\n    mutate(across(\n        c(\n            ParentSubstanceUse, ParentIncarceration, SchoolConnectedness,\n            ParentalMonitoring, UnfairDisciplineAtSchool\n        ),\n        ~ as.numeric(.x) - 1\n    )) %&gt;%\n    mutate(across(\n        c(\n            ParentSubstanceUse, ParentIncarceration, SchoolConnectedness,\n            ParentalMonitoring, UnfairDisciplineAtSchool\n        ),\n        ~ factor(.x)\n    ))\n\n\n\n\n\n\n\n\nanalysis_folds &lt;- vfold_cv(analysis_train,\n    v = 5\n)\nanalysis_folds"
  },
  {
    "objectID": "models/datasets.html#youth-risk-behavior-survey-2023",
    "href": "models/datasets.html#youth-risk-behavior-survey-2023",
    "title": "Data and Research Question",
    "section": "",
    "text": "The Youth Risk Behavior Survey (YRBS) is a national survey that monitors health-related behaviors among high school students, including weapon carrying and associated risk factors.\n\n\n\nSource: Centers for Disease Control and Prevention (CDC)\nYear: 2023\nTarget Population: High school students\nSample Size: Approximately 19,000 students nationwide\n\n\n\n\nHow do logistic regression, lasso, k-nearest neighbors, and tree-based models compare in predicting school-based weapon carrying among adolescents based on risk and protective factors?\n\n\n\nThe dataset includes information on various health-related behaviors:\n\nOutcome\n\nWeapon Carrying (Carried a weapon on school property)\n\nPredictors\n\nTraumatic experiences\nSchool Safety Perceptions\nBullying Experiences\nFamily Support\nSocial Media Use\nPeer Relationships\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(dissertationData)\nlibrary(here)\ndata(clean_yrbs_2023)\n# Add your data preprocessing code here\n\n\n\n\nWe will do it in classâ€¦\n\n\n\n\n# This is an example of how to create a dataset for a model.\n# You can use this as a template to create your own dataset.\n\n\nanalysis_data &lt;- clean_yrbs_2023 %&gt;%\n    select(\n        WeaponCarryingSchool, AttackedInNeighborhood, Bullying,\n        SexualAbuseByOlderPerson, ParentalPhysicalAbuse, ParentSubstanceUse,\n        ParentIncarceration, SchoolConnectedness, ParentalMonitoring,\n        UnfairDisciplineAtSchool, Homelessness\n    ) |&gt;\n    filter(!is.na(WeaponCarryingSchool)) %&gt;%\n    mutate(across(\n        c(\n            ParentSubstanceUse, ParentIncarceration, SchoolConnectedness,\n            ParentalMonitoring, UnfairDisciplineAtSchool\n        ),\n        ~ as.numeric(.x) - 1\n    )) %&gt;%\n    mutate(across(\n        c(\n            ParentSubstanceUse, ParentIncarceration, SchoolConnectedness,\n            ParentalMonitoring, UnfairDisciplineAtSchool\n        ),\n        ~ factor(.x)\n    ))\n\n\n\n\n\n\n\n\nanalysis_folds &lt;- vfold_cv(analysis_train,\n    v = 5\n)\nanalysis_folds"
  },
  {
    "objectID": "models/lasso.html",
    "href": "models/lasso.html",
    "title": "Lasso Regression",
    "section": "",
    "text": "Lasso regression is a statistical model that combines linear/logistic regression with L1 regularization to perform both variable selection and regularization. The term â€œLassoâ€ stands for â€œLeast Absolute Shrinkage and Selection Operator.â€ This method is particularly useful when dealing with datasets that have many predictors, as it helps to: - Reduce overfitting by penalizing large coefficients - Perform automatic feature selection by shrinking some coefficients to exactly zero - Handle multicollinearity by selecting only one variable from a group of highly correlated predictors\nIn this analysis, weâ€™ll use Lasso regression to predict weapon carrying behavior in schools, demonstrating how this method can help identify the most important predictors while maintaining model interpretability."
  },
  {
    "objectID": "models/lasso.html#setting-up-the-environment",
    "href": "models/lasso.html#setting-up-the-environment",
    "title": "Lasso Regression",
    "section": "Setting Up the Environment",
    "text": "Setting Up the Environment\nFirst, we need to load the necessary packages for our analysis. Weâ€™ll use tidymodels for modeling, tidyverse for data manipulation, and here for consistent file paths.\n\nlibrary(here)\nlibrary(tidymodels)\nlibrary(tidyverse)"
  },
  {
    "objectID": "models/lasso.html#loading-the-data",
    "href": "models/lasso.html#loading-the-data",
    "title": "Lasso Regression",
    "section": "Loading the Data",
    "text": "Loading the Data\nWeâ€™ll work with pre-processed data sets that have been split into training and test sets, along with cross-validation folds. These files are stored in the processed_data directory.\n\nanalysis_data &lt;- readRDS(here(\"models\", \"data\", \"analysis_data.rds\"))\nanalysis_train &lt;- readRDS(here(\"models\", \"data\", \"analysis_train.rds\"))\nanalysis_test &lt;- readRDS(here(\"models\",\"data\", \"analysis_test.rds\"))\nanalysis_folds &lt;- readRDS(here(\"models\", \"data\", \"analysis_folds.rds\"))"
  },
  {
    "objectID": "models/lasso.html#data-preprocessing",
    "href": "models/lasso.html#data-preprocessing",
    "title": "Lasso Regression",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nBefore fitting our model, we need to preprocess the data. Weâ€™ll create a recipe that: - Imputes missing values in categorical variables using the mode - Imputes missing values in numeric variables using the mean - Removes predictors with zero variance - Removes highly correlated predictors (correlation threshold = 0.7) - Creates dummy variables for categorical predictors\n\nweapon_carry_recipe &lt;- \n  recipe(formula = WeaponCarryingSchool ~ ., data = analysis_train) |&gt;\n  step_impute_mode(all_nominal_predictors()) |&gt;\n  step_impute_mean(all_numeric_predictors()) |&gt;\n  step_zv(all_predictors()) |&gt; \n  step_corr(all_numeric_predictors(), threshold = 0.7) %&gt;% \n  step_dummy(all_nominal_predictors())\n\nweapon_carry_recipe\n\n\n\n\nâ”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n\n\n\n\nâ”€â”€ Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 10\n\n\n\n\n\nâ”€â”€ Operations \n\n\nâ€¢ Mode imputation for: all_nominal_predictors()\n\n\nâ€¢ Mean imputation for: all_numeric_predictors()\n\n\nâ€¢ Zero variance filter on: all_predictors()\n\n\nâ€¢ Correlation filter on: all_numeric_predictors()\n\n\nâ€¢ Dummy variables from: all_nominal_predictors()\n\n\nLetâ€™s apply our recipe to transform the data according to these preprocessing steps.\n\nweapon_carry_recipe %&gt;% \n  prep() %&gt;% \n  bake(new_data = analysis_data) \n\n# A tibble: 19,595 Ã— 11\n   WeaponCarryingSchool AttackedInNeighborhood_X1 Bullying_X1\n   &lt;fct&gt;                                    &lt;dbl&gt;       &lt;dbl&gt;\n 1 0                                            0           0\n 2 0                                            0           1\n 3 0                                            0           0\n 4 0                                            0           0\n 5 0                                            0           0\n 6 0                                            1           0\n 7 0                                            0           0\n 8 0                                            0           0\n 9 0                                            0           0\n10 0                                            0           0\n# â„¹ 19,585 more rows\n# â„¹ 8 more variables: SexualAbuseByOlderPerson_X1 &lt;dbl&gt;,\n#   ParentalPhysicalAbuse_X1 &lt;dbl&gt;, ParentSubstanceUse_X1 &lt;dbl&gt;,\n#   ParentIncarceration_X1 &lt;dbl&gt;, SchoolConnectedness_X1 &lt;dbl&gt;,\n#   ParentalMonitoring_X1 &lt;dbl&gt;, UnfairDisciplineAtSchool_X1 &lt;dbl&gt;,\n#   Homelessness_X1 &lt;dbl&gt;"
  },
  {
    "objectID": "models/lasso.html#model-specification",
    "href": "models/lasso.html#model-specification",
    "title": "Lasso Regression",
    "section": "Model Specification",
    "text": "Model Specification\nWeâ€™ll use a logistic regression model with Lasso regularization. The Lasso (Least Absolute Shrinkage and Selection Operator) helps with feature selection by penalizing the absolute size of coefficients. We set mixture = 1 to specify a pure Lasso model, and weâ€™ll tune the penalty parameter to find the optimal level of regularization.\n\nweapon_carry_spec &lt;-\n  logistic_reg(penalty = tune(), \n               mixture = 1) |&gt; \n  set_engine('glmnet')\n\nweapon_carry_spec\n\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet"
  },
  {
    "objectID": "models/lasso.html#creating-the-workflow",
    "href": "models/lasso.html#creating-the-workflow",
    "title": "Lasso Regression",
    "section": "Creating the Workflow",
    "text": "Creating the Workflow\nWeâ€™ll combine our recipe and model specification into a single workflow. This ensures that all preprocessing steps are properly applied during both training and prediction.\n\nweapon_carry_workflow &lt;-\n  workflow() |&gt;\n  add_recipe(weapon_carry_recipe) |&gt;\n  add_model(weapon_carry_spec)\n\nweapon_carry_workflow\n\nâ•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nPreprocessor: Recipe\nModel: logistic_reg()\n\nâ”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n5 Recipe Steps\n\nâ€¢ step_impute_mode()\nâ€¢ step_impute_mean()\nâ€¢ step_zv()\nâ€¢ step_corr()\nâ€¢ step_dummy()\n\nâ”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet"
  },
  {
    "objectID": "models/lasso.html#model-tuning",
    "href": "models/lasso.html#model-tuning",
    "title": "Lasso Regression",
    "section": "Model Tuning",
    "text": "Model Tuning\nTo find the optimal penalty value, weâ€™ll create a grid of potential values to test. Weâ€™ll use 50 different penalty values, evenly spaced on a logarithmic scale.\n\nlambda_grid &lt;- grid_regular(penalty(), levels = 50)\nlambda_grid\n\n# A tibble: 50 Ã— 1\n    penalty\n      &lt;dbl&gt;\n 1 1   e-10\n 2 1.60e-10\n 3 2.56e-10\n 4 4.09e-10\n 5 6.55e-10\n 6 1.05e- 9\n 7 1.68e- 9\n 8 2.68e- 9\n 9 4.29e- 9\n10 6.87e- 9\n# â„¹ 40 more rows\n\n\nNow, weâ€™ll perform cross-validation to find the best penalty value. This process is time-consuming, so weâ€™ll save the results for future use.\n\nset.seed(2023)\n\nlasso_tune &lt;- \n  tune_grid(\n  object = weapon_carry_workflow, \n  resamples = analysis_folds,\n  grid = lambda_grid, \n  control = control_resamples(event_level = \"second\")\n)\n\nLetâ€™s examine the performance metrics for different penalty values.\n\nlasso_tune %&gt;% \n  collect_metrics()\n\n# A tibble: 150 Ã— 7\n    penalty .metric     .estimator  mean     n std_err .config              \n      &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1 1   e-10 accuracy    binary     0.957     5 0.00158 Preprocessor1_Model01\n 2 1   e-10 brier_class binary     0.881     5 0.00160 Preprocessor1_Model01\n 3 1   e-10 roc_auc     binary     0.688     5 0.00742 Preprocessor1_Model01\n 4 1.60e-10 accuracy    binary     0.957     5 0.00158 Preprocessor1_Model02\n 5 1.60e-10 brier_class binary     0.881     5 0.00160 Preprocessor1_Model02\n 6 1.60e-10 roc_auc     binary     0.688     5 0.00742 Preprocessor1_Model02\n 7 2.56e-10 accuracy    binary     0.957     5 0.00158 Preprocessor1_Model03\n 8 2.56e-10 brier_class binary     0.881     5 0.00160 Preprocessor1_Model03\n 9 2.56e-10 roc_auc     binary     0.688     5 0.00742 Preprocessor1_Model03\n10 4.09e-10 accuracy    binary     0.957     5 0.00158 Preprocessor1_Model04\n# â„¹ 140 more rows\n\n\nWe can visualize how the modelâ€™s performance changes with different penalty values.\n\nautoplot(lasso_tune)"
  },
  {
    "objectID": "models/lasso.html#selecting-the-best-model",
    "href": "models/lasso.html#selecting-the-best-model",
    "title": "Lasso Regression",
    "section": "Selecting the Best Model",
    "text": "Selecting the Best Model\nWeâ€™ll select the best model based on the ROC AUC metric, which measures the modelâ€™s ability to distinguish between classes.\n\nbest &lt;- lasso_tune |&gt; \n  select_best(metric =\"roc_auc\")\n\nbest\n\n# A tibble: 1 Ã— 2\n   penalty .config              \n     &lt;dbl&gt; &lt;chr&gt;                \n1 0.000339 Preprocessor1_Model33\n\n\nNow weâ€™ll create our final workflow with the best penalty value.\n\nfinal_wf &lt;- finalize_workflow(weapon_carry_workflow, best)\n\nfinal_wf\n\nâ•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nPreprocessor: Recipe\nModel: logistic_reg()\n\nâ”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n5 Recipe Steps\n\nâ€¢ step_impute_mode()\nâ€¢ step_impute_mean()\nâ€¢ step_zv()\nâ€¢ step_corr()\nâ€¢ step_dummy()\n\nâ”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = 0.000339322177189533\n  mixture = 1\n\nComputational engine: glmnet"
  },
  {
    "objectID": "models/lasso.html#fitting-the-final-model",
    "href": "models/lasso.html#fitting-the-final-model",
    "title": "Lasso Regression",
    "section": "Fitting the Final Model",
    "text": "Fitting the Final Model\nWeâ€™ll fit our final model on the training data. This process is also time-consuming, so weâ€™ll save the results.\n\nweapon_fit &lt;- \n  fit(final_wf, data = analysis_train)\n\nweapon_fit"
  },
  {
    "objectID": "models/lasso.html#model-evaluation",
    "href": "models/lasso.html#model-evaluation",
    "title": "Lasso Regression",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nLetâ€™s examine the modelâ€™s predictions on the training data.\n\nweapon_pred &lt;- \n  augment(weapon_fit, analysis_train) |&gt; \n  select(WeaponCarryingSchool, .pred_class, .pred_1, .pred_0)\n\nweapon_pred\n\n# A tibble: 14,696 Ã— 4\n   WeaponCarryingSchool .pred_class .pred_1 .pred_0\n   &lt;fct&gt;                &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n 1 0                    0            0.0368   0.963\n 2 0                    0            0.0410   0.959\n 3 0                    0            0.0253   0.975\n 4 0                    0            0.0325   0.968\n 5 0                    0            0.125    0.875\n 6 0                    0            0.0368   0.963\n 7 0                    0            0.0208   0.979\n 8 0                    0            0.0208   0.979\n 9 0                    0            0.0458   0.954\n10 0                    0            0.0529   0.947\n# â„¹ 14,686 more rows\n\n\nWe can visualize the modelâ€™s performance using an ROC curve.\n\nroc_plot_training &lt;- \n  weapon_pred |&gt; \n  roc_curve(truth = WeaponCarryingSchool, .pred_1, event_level = \"second\") |&gt; \n  autoplot()\n\nroc_plot_training \n\n\n\n\n\n\n\n\nLetâ€™s look at the model coefficients to understand which predictors are most important.\n\nweapon_fit |&gt; \n  extract_fit_parsnip() |&gt; \n  tidy()\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-9\n\n\n# A tibble: 11 Ã— 3\n   term                        estimate  penalty\n   &lt;chr&gt;                          &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)                  -3.29   0.000339\n 2 AttackedInNeighborhood_X1     0.739  0.000339\n 3 Bullying_X1                   0.472  0.000339\n 4 SexualAbuseByOlderPerson_X1   0.455  0.000339\n 5 ParentalPhysicalAbuse_X1      0.708  0.000339\n 6 ParentSubstanceUse_X1        -0.132  0.000339\n 7 ParentIncarceration_X1        0.0271 0.000339\n 8 SchoolConnectedness_X1       -0.226  0.000339\n 9 ParentalMonitoring_X1         0.584  0.000339\n10 UnfairDisciplineAtSchool_X1  -0.229  0.000339\n11 Homelessness_X1               1.17   0.000339"
  },
  {
    "objectID": "models/lasso.html#cross-validation-results",
    "href": "models/lasso.html#cross-validation-results",
    "title": "Lasso Regression",
    "section": "Cross-Validation Results",
    "text": "Cross-Validation Results\nWeâ€™ll fit the model on each cross-validation fold to get a more robust estimate of its performance.\n\nweapon_fit_resamples &lt;- \n  fit_resamples(final_wf, resamples = analysis_folds)\n\nweapon_fit_resamples\n\nLetâ€™s examine the cross-validation metrics.\n\ncollect_metrics(weapon_fit_resamples)\n\n# A tibble: 3 Ã— 6\n  .metric     .estimator   mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.957      5 0.00158 Preprocessor1_Model1\n2 brier_class binary     0.0399     5 0.00128 Preprocessor1_Model1\n3 roc_auc     binary     0.688      5 0.00737 Preprocessor1_Model1"
  },
  {
    "objectID": "models/lasso.html#variable-importance",
    "href": "models/lasso.html#variable-importance",
    "title": "Lasso Regression",
    "section": "Variable Importance",
    "text": "Variable Importance\nFinally, letâ€™s create a variable importance plot to identify the most influential predictors in our model.\n\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nweapon_fit |&gt; \n  extract_fit_engine() |&gt; \n  vip()"
  },
  {
    "objectID": "models/lasso.html#results-and-interpretation",
    "href": "models/lasso.html#results-and-interpretation",
    "title": "Lasso Regression",
    "section": "Results and Interpretation",
    "text": "Results and Interpretation\n[Add your interpretation of the results here]"
  },
  {
    "objectID": "models/logistic.html",
    "href": "models/logistic.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Logistic regression is a statistical model thatâ€¦\n\nModel Overview\nLogistic regression is used when the dependent variable is binary (0/1, Yes/No, True/False). The model estimates the probability of the dependent variable being 1 given the independent variables.\n\n\nImplementation\n\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(dissertationData)\nlibrary(here)\n\n# Load and prepare the YRBS 2023 dataset\n\n\n\nLoad the data\n\nanalysis_data &lt;- readRDS(here(\"models\", \"data\", \"analysis_data.rds\"))\nanalysis_train &lt;- readRDS(here(\"models\", \"data\", \"analysis_train.rds\"))\nanalysis_test &lt;- readRDS(here(\"models\", \"data\", \"analysis_test.rds\"))\nanalysis_folds &lt;- readRDS(here(\"models\", \"data\", \"analysis_folds.rds\"))\n\n\n\nRecipe\n\nweapon_carry_recipe &lt;- \n  recipe(formula = WeaponCarryingSchool ~ ., data = analysis_data) |&gt;\n  step_impute_mode(all_nominal_predictors()) |&gt;\n  step_impute_mean(all_numeric_predictors()) |&gt;\n  step_zv(all_predictors()) |&gt; \n  step_corr(all_numeric_predictors(), threshold = 0.7) \n\nweapon_carry_recipe\n\n\n\n\nâ”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n\n\n\n\nâ”€â”€ Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 10\n\n\n\n\n\nâ”€â”€ Operations \n\n\nâ€¢ Mode imputation for: all_nominal_predictors()\n\n\nâ€¢ Mean imputation for: all_numeric_predictors()\n\n\nâ€¢ Zero variance filter on: all_predictors()\n\n\nâ€¢ Correlation filter on: all_numeric_predictors()\n\n\n\n\nBake\n\nrec &lt;- weapon_carry_recipe %&gt;% \n  prep() %&gt;% \n  bake(new_data = analysis_data) %&gt;% glimpse()\n\nRows: 19,595\nColumns: 11\n$ AttackedInNeighborhood   &lt;fct&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, â€¦\n$ Bullying                 &lt;fct&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n$ SexualAbuseByOlderPerson &lt;fct&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, â€¦\n$ ParentalPhysicalAbuse    &lt;fct&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n$ ParentSubstanceUse       &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, â€¦\n$ ParentIncarceration      &lt;fct&gt; 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, â€¦\n$ SchoolConnectedness      &lt;fct&gt; 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, â€¦\n$ ParentalMonitoring       &lt;fct&gt; 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, â€¦\n$ UnfairDisciplineAtSchool &lt;fct&gt; 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, â€¦\n$ Homelessness             &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n$ WeaponCarryingSchool     &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n\n\n\n\nModel Specification\n\nweapon_carry_spec &lt;- \n  logistic_reg() %&gt;% \n  set_mode(\"classification\") %&gt;% \n  set_engine(\"glm\") \n\nweapon_carry_spec\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\n\nWorkflow\n\nweapon_carry_workflow &lt;- workflow() %&gt;%\n  add_recipe(weapon_carry_recipe) %&gt;%\n  add_model(weapon_carry_spec)\n\n\nweapon_carry_workflow\n\nâ•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nPreprocessor: Recipe\nModel: logistic_reg()\n\nâ”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n4 Recipe Steps\n\nâ€¢ step_impute_mode()\nâ€¢ step_impute_mean()\nâ€¢ step_zv()\nâ€¢ step_corr()\n\nâ”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\nmod_1 &lt;- \n  fit(weapon_carry_workflow, data = analysis_train) \n\nmod_1\n\nâ•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nPreprocessor: Recipe\nModel: logistic_reg()\n\nâ”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n4 Recipe Steps\n\nâ€¢ step_impute_mode()\nâ€¢ step_impute_mean()\nâ€¢ step_zv()\nâ€¢ step_corr()\n\nâ”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n              (Intercept)    AttackedInNeighborhood1  \n                 -3.29938                    0.74950  \n                Bullying1  SexualAbuseByOlderPerson1  \n                  0.48405                    0.46540  \n   ParentalPhysicalAbuse1        ParentSubstanceUse1  \n                  0.71713                   -0.15917  \n     ParentIncarceration1       SchoolConnectedness1  \n                  0.07048                   -0.25542  \n      ParentalMonitoring1  UnfairDisciplineAtSchool1  \n                  0.59860                   -0.24268  \n            Homelessness1  \n                  1.18053  \n\nDegrees of Freedom: 14695 Total (i.e. Null);  14685 Residual\nNull Deviance:      5238 \nResidual Deviance: 4872     AIC: 4894\n\n\n\ntidy_model &lt;- \n  mod_1 |&gt;\n  tidy(exponentiate = TRUE,\n       conf.int = TRUE, \n       conf.level = .95) |&gt;\n  mutate(p.value = scales::pvalue(p.value))\n\ntidy_model\n\n# A tibble: 11 Ã— 7\n   term                  estimate std.error statistic p.value conf.low conf.high\n   &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)             0.0369    0.166    -19.9   &lt;0.001    0.0266    0.0508\n 2 AttackedInNeighborhoâ€¦   2.12      0.0954     7.85  &lt;0.001    1.75      2.55  \n 3 Bullying1               1.62      0.0919     5.27  &lt;0.001    1.35      1.94  \n 4 SexualAbuseByOlderPeâ€¦   1.59      0.133      3.51  &lt;0.001    1.22      2.06  \n 5 ParentalPhysicalAbusâ€¦   2.05      0.179      4.01  &lt;0.001    1.43      2.89  \n 6 ParentSubstanceUse1     0.853     0.111     -1.44  0.151     0.688     1.06  \n 7 ParentIncarceration1    1.07      0.126      0.560 0.575     0.841     1.38  \n 8 SchoolConnectedness1    0.775     0.0970    -2.63  0.008     0.639     0.935 \n 9 ParentalMonitoring1     1.82      0.114      5.26  &lt;0.001    1.45      2.27  \n10 UnfairDisciplineAtScâ€¦   0.785     0.114     -2.13  0.033     0.629     0.984 \n11 Homelessness1           3.26      0.155      7.61  &lt;0.001    2.39      4.39  \n\n\n\n\nModel Evaluation\n\nweapon_pred &lt;- \n  augment(mod_1, analysis_train) |&gt; \n  select(WeaponCarryingSchool, .pred_class, .pred_1, .pred_0)\n\nweapon_pred\n\n# A tibble: 14,696 Ã— 4\n   WeaponCarryingSchool .pred_class .pred_1 .pred_0\n   &lt;fct&gt;                &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n 1 0                    0            0.0360   0.964\n 2 0                    0            0.0412   0.959\n 3 0                    0            0.0241   0.976\n 4 0                    0            0.0317   0.968\n 5 0                    0            0.128    0.872\n 6 0                    0            0.0360   0.964\n 7 0                    0            0.0201   0.980\n 8 0                    0            0.0201   0.980\n 9 0                    0            0.0471   0.953\n10 0                    0            0.0531   0.947\n# â„¹ 14,686 more rows\n\n\n\nroc_plot_training &lt;- \n  weapon_pred |&gt; \n  roc_curve(truth = WeaponCarryingSchool, .pred_1, event_level = \"second\") |&gt; \n  autoplot()\n\nroc_plot_training \n\n\n\n\n\n\n\n\n\n\nVisualizations\n\ntidy_model |&gt; \n  filter(term != \"(Intercept)\") |&gt; \n  ggplot(aes(x = estimate, y = reorder(term, estimate))) +\n  geom_point(size = 3) +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.2) +\n  geom_vline(xintercept = 1, linetype = \"dashed\", color = \"red\") +\n  scale_x_log10() +\n  labs(\n    x = \"Odds Ratio (log scale)\",\n    y = \"Predictors\",\n    title = \"Forest Plot of Logistic Regression Coefficients\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.y = element_text(size = 10),\n    plot.title = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\n\nResults and Interpretation\n\n\nKey Takeaways"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Notebook ğŸ‘€",
    "section": "",
    "text": "This website will be our Machine Learning course, where we explore various statistical and machine learning models using the Youth Risk Behavior Survey (YRBS) 2023 dataset. Through this course, I learned to implement and understand different modeling techniques using the tidymodels framework in R.\n\nWhat Youâ€™ll Find Here\n\nAbout Me: A little bit about me and my research interests.\nThe Dataset: Information about my primary dataset and the research question I am trying to answer.\nModel Implementations: Hands-on examples of different models to answer the question:\n\nLogistic Regression\nLasso Regression\nDecision Trees\nRandom Forest\nK-Nearest Neighbors (KNN)"
  },
  {
    "objectID": "models/trees.html",
    "href": "models/trees.html",
    "title": "Classification Tree",
    "section": "",
    "text": "A decision tree is a non-parametric supervised learning algorithm, which is utilized for both classification and regression tasks. It has a hierarchical, tree structure, which consists of a root node, branches, internal nodes and leaf nodes."
  },
  {
    "objectID": "models/trees.html#libraries",
    "href": "models/trees.html#libraries",
    "title": "Classification Tree",
    "section": "Libraries",
    "text": "Libraries\n\nlibrary(tidymodels)"
  },
  {
    "objectID": "models/trees.html#data",
    "href": "models/trees.html#data",
    "title": "Classification Tree",
    "section": "Data",
    "text": "Data\n\nanalysis_train &lt;- readRDS(\"data/analysis_train.rds\")\nanalysis_folds &lt;- readRDS(\"data/analysis_folds.rds\")"
  },
  {
    "objectID": "models/trees.html#model-recipe",
    "href": "models/trees.html#model-recipe",
    "title": "Classification Tree",
    "section": "Model Recipe",
    "text": "Model Recipe\n\ntree_recipe &lt;-\n  recipe(WeaponCarryingSchool ~ ., data = analysis_train) |&gt; \n  step_impute_mode(all_nominal_predictors()) |&gt; \n  step_impute_mean(all_numeric_predictors())\n\ntree_recipe\n\n\n\n\nâ”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n\n\n\n\nâ”€â”€ Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 10\n\n\n\n\n\nâ”€â”€ Operations \n\n\nâ€¢ Mode imputation for: all_nominal_predictors()\n\n\nâ€¢ Mean imputation for: all_numeric_predictors()"
  },
  {
    "objectID": "models/trees.html#model-specification",
    "href": "models/trees.html#model-specification",
    "title": "Classification Tree",
    "section": "Model Specification",
    "text": "Model Specification\n\ntree_spec &lt;- \n  decision_tree(\n    cost_complexity = tune(),\n    tree_depth = tune(),\n    min_n = tune(),\n  ) |&gt; \n  set_engine(\"rpart\") |&gt; \n  set_mode(\"classification\")\n\ntree_spec\n\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n  min_n = tune()\n\nComputational engine: rpart"
  },
  {
    "objectID": "models/trees.html#model-workflow",
    "href": "models/trees.html#model-workflow",
    "title": "Classification Tree",
    "section": "Model Workflow",
    "text": "Model Workflow\n\ntree_workflow &lt;-\n  workflow() |&gt; \n  add_recipe(tree_recipe) |&gt; \n  add_model(tree_spec)\n\ntree_workflow\n\nâ•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nPreprocessor: Recipe\nModel: decision_tree()\n\nâ”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n2 Recipe Steps\n\nâ€¢ step_impute_mode()\nâ€¢ step_impute_mean()\n\nâ”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n  min_n = tune()\n\nComputational engine: rpart"
  },
  {
    "objectID": "models/trees.html#grid-search-cross-validation",
    "href": "models/trees.html#grid-search-cross-validation",
    "title": "Classification Tree",
    "section": "Grid Search Cross Validation",
    "text": "Grid Search Cross Validation\n\ntree_grid &lt;- grid_regular(\n  cost_complexity(),\n  tree_depth(c(2,5)),\n  min_n(),\n  levels = 2\n)\n\ntree_grid\n\n# A tibble: 8 Ã— 3\n  cost_complexity tree_depth min_n\n            &lt;dbl&gt;      &lt;int&gt; &lt;int&gt;\n1    0.0000000001          2     2\n2    0.1                   2     2\n3    0.0000000001          5     2\n4    0.1                   5     2\n5    0.0000000001          2    40\n6    0.1                   2    40\n7    0.0000000001          5    40\n8    0.1                   5    40"
  },
  {
    "objectID": "models/trees.html#hyperparameter-tuning",
    "href": "models/trees.html#hyperparameter-tuning",
    "title": "Classification Tree",
    "section": "Hyperparameter Tuning",
    "text": "Hyperparameter Tuning\n\ntree_tune &lt;-\n  tree_workflow |&gt; \n  tune_grid(\n    resamples = analysis_folds,\n    grid = tree_grid,\n    metrics = metric_set(roc_auc),\n    control = control_grid(save_pred = TRUE)\n  )\n\nsaveRDS(tree_tune, \"model_outputs/tree_tune.rds\")\n\ntree_tune\n\n\n\n# Tuning results\n# 5-fold cross-validation \n# A tibble: 5 Ã— 5\n  splits               id    .metrics         .notes           .predictions\n  &lt;list&gt;               &lt;chr&gt; &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n1 &lt;split [11756/2940]&gt; Fold1 &lt;tibble [8 Ã— 7]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n2 &lt;split [11757/2939]&gt; Fold2 &lt;tibble [8 Ã— 7]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n3 &lt;split [11757/2939]&gt; Fold3 &lt;tibble [8 Ã— 7]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n4 &lt;split [11757/2939]&gt; Fold4 &lt;tibble [8 Ã— 7]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n5 &lt;split [11757/2939]&gt; Fold5 &lt;tibble [8 Ã— 7]&gt; &lt;tibble [0 Ã— 3]&gt; &lt;tibble&gt;    \n\n\n\nshow_best(tree_tune, metric = \"roc_auc\")\n\n# A tibble: 5 Ã— 9\n  cost_complexity tree_depth min_n .metric .estimator  mean     n std_err\n            &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1    0.0000000001          5     2 roc_auc binary     0.592     5 0.0226 \n2    0.0000000001          5    40 roc_auc binary     0.534     5 0.0167 \n3    0.0000000001          2     2 roc_auc binary     0.507     5 0.00740\n4    0.0000000001          2    40 roc_auc binary     0.507     5 0.00740\n5    0.1                   2     2 roc_auc binary     0.5       5 0      \n# â„¹ 1 more variable: .config &lt;chr&gt;\n\n\n\nbest_plot_tree &lt;- autoplot(tree_tune)\n\nbest_plot_tree\n\n\n\n\n\n\n\nFigureÂ 1\n\n\n\n\n\n\nbest_tree &lt;- select_best(tree_tune, metric = \"roc_auc\")\n\nbest_tree\n\n# A tibble: 1 Ã— 4\n  cost_complexity tree_depth min_n .config             \n            &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;               \n1    0.0000000001          5     2 Preprocessor1_Model3"
  },
  {
    "objectID": "models/trees.html#finalize-model-workflow",
    "href": "models/trees.html#finalize-model-workflow",
    "title": "Classification Tree",
    "section": "Finalize Model Workflow",
    "text": "Finalize Model Workflow\n\ntree_final_workflow &lt;- finalize_workflow(tree_workflow, best_tree)\n\ntree_final_workflow\n\nâ•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nPreprocessor: Recipe\nModel: decision_tree()\n\nâ”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n2 Recipe Steps\n\nâ€¢ step_impute_mode()\nâ€¢ step_impute_mean()\n\nâ”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = 1e-10\n  tree_depth = 5\n  min_n = 2\n\nComputational engine: rpart"
  },
  {
    "objectID": "models/trees.html#fit-the-model",
    "href": "models/trees.html#fit-the-model",
    "title": "Classification Tree",
    "section": "Fit the Model",
    "text": "Fit the Model\n\ntree_fit &lt;- fit(tree_final_workflow, analysis_train)\n\nsaveRDS(tree_fit, \"model_outputs/tree_fit.rds\")\n\ntree_fit\n\n\n\nâ•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nPreprocessor: Recipe\nModel: decision_tree()\n\nâ”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n2 Recipe Steps\n\nâ€¢ step_impute_mode()\nâ€¢ step_impute_mean()\n\nâ”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nn= 14696 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 14696 636 0 (0.95672292 0.04327708)  \n   2) Homelessness=0 14336 567 0 (0.96044922 0.03955078) *\n   3) Homelessness=1 360  69 0 (0.80833333 0.19166667)  \n     6) SexualAbuseByOlderPerson=0 298  45 0 (0.84899329 0.15100671)  \n      12) AttackedInNeighborhood=0 208  24 0 (0.88461538 0.11538462) *\n      13) AttackedInNeighborhood=1 90  21 0 (0.76666667 0.23333333)  \n        26) SchoolConnectedness=1 38   5 0 (0.86842105 0.13157895) *\n        27) SchoolConnectedness=0 52  16 0 (0.69230769 0.30769231)  \n          54) ParentalPhysicalAbuse=0 47  12 0 (0.74468085 0.25531915) *\n          55) ParentalPhysicalAbuse=1 5   1 1 (0.20000000 0.80000000) *\n     7) SexualAbuseByOlderPerson=1 62  24 0 (0.61290323 0.38709677)  \n      14) ParentIncarceration=1 30   8 0 (0.73333333 0.26666667) *\n      15) ParentIncarceration=0 32  16 0 (0.50000000 0.50000000)  \n        30) ParentalPhysicalAbuse=0 15   6 0 (0.60000000 0.40000000) *\n        31) ParentalPhysicalAbuse=1 17   7 1 (0.41176471 0.58823529)  \n          62) SchoolConnectedness=0 8   3 0 (0.62500000 0.37500000) *\n          63) SchoolConnectedness=1 9   2 1 (0.22222222 0.77777778) *"
  },
  {
    "objectID": "models/trees.html#make-model-predictions",
    "href": "models/trees.html#make-model-predictions",
    "title": "Classification Tree",
    "section": "Make Model Predictions",
    "text": "Make Model Predictions\n\ntree_pred &lt;- \n  augment(tree_fit, analysis_train) |&gt; \n  select(WeaponCarryingSchool, .pred_class, .pred_1, .pred_0)\n\ntree_pred\n\n# A tibble: 14,696 Ã— 4\n   WeaponCarryingSchool .pred_class .pred_1 .pred_0\n   &lt;fct&gt;                &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n 1 0                    0            0.0396   0.960\n 2 0                    0            0.0396   0.960\n 3 0                    0            0.0396   0.960\n 4 0                    0            0.0396   0.960\n 5 0                    0            0.0396   0.960\n 6 0                    0            0.0396   0.960\n 7 0                    0            0.0396   0.960\n 8 0                    0            0.0396   0.960\n 9 0                    0            0.0396   0.960\n10 0                    0            0.0396   0.960\n# â„¹ 14,686 more rows"
  },
  {
    "objectID": "models/trees.html#roc-plot",
    "href": "models/trees.html#roc-plot",
    "title": "Classification Tree",
    "section": "ROC Plot",
    "text": "ROC Plot\n\nroc_tree &lt;- \n  tree_pred |&gt; \n  roc_curve(\n    truth = WeaponCarryingSchool,\n    .pred_1,\n    event_level = \"second\"\n  ) |&gt; \n  autoplot()\n\nsaveRDS(roc_tree, \"roc_graphs/tree.rds\")\n\nroc_tree\n\n\n\n\n\n\n\n\n\n\n\ntree_pred |&gt; \n  roc_auc(\n    truth = WeaponCarryingSchool,\n    .pred_1,\n    event_level = \"second\"\n  )\n\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.544"
  },
  {
    "objectID": "models/trees.html#resample-metrics",
    "href": "models/trees.html#resample-metrics",
    "title": "Classification Tree",
    "section": "Resample Metrics",
    "text": "Resample Metrics\n\nfit_resamples(tree_final_workflow, resamples = analysis_folds) |&gt; \n  collect_metrics()\n\n# A tibble: 3 Ã— 6\n  .metric     .estimator   mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.955      5 0.00141 Preprocessor1_Model1\n2 brier_class binary     0.0412     5 0.00135 Preprocessor1_Model1\n3 roc_auc     binary     0.592      5 0.0226  Preprocessor1_Model1"
  },
  {
    "objectID": "models/trees.html#decision-tree-plots",
    "href": "models/trees.html#decision-tree-plots",
    "title": "Classification Tree",
    "section": "Decision Tree Plots",
    "text": "Decision Tree Plots\n\ntree_fit |&gt; \n  extract_fit_engine() |&gt; \n  rpart.plot::rpart.plot(roundint = FALSE)\n\n\n\n\n\n\n\nFigureÂ 2"
  },
  {
    "objectID": "models/trees.html#model-overview",
    "href": "models/trees.html#model-overview",
    "title": "Classification Tree",
    "section": "",
    "text": "A decision tree is a non-parametric supervised learning algorithm, which is utilized for both classification and regression tasks. It has a hierarchical, tree structure, which consists of a root node, branches, internal nodes and leaf nodes."
  },
  {
    "objectID": "models/random-forest.html",
    "href": "models/random-forest.html",
    "title": "random-forest",
    "section": "",
    "text": "Random forest is a commonly-used machine learning algorithm, trademarked by Leo Breiman and Adele Cutler, that combines the output of multiple decision trees to reach a single result. Its ease of use and flexibility have fueled its adoption, as it handles both classification and regression problems."
  },
  {
    "objectID": "models/random-forest.html#model-overview",
    "href": "models/random-forest.html#model-overview",
    "title": "random-forest",
    "section": "",
    "text": "Random forest is a commonly-used machine learning algorithm, trademarked by Leo Breiman and Adele Cutler, that combines the output of multiple decision trees to reach a single result. Its ease of use and flexibility have fueled its adoption, as it handles both classification and regression problems."
  },
  {
    "objectID": "models/random-forest.html#libraries",
    "href": "models/random-forest.html#libraries",
    "title": "random-forest",
    "section": "Libraries",
    "text": "Libraries\n\nlibrary(tidymodels)"
  },
  {
    "objectID": "models/random-forest.html#data",
    "href": "models/random-forest.html#data",
    "title": "random-forest",
    "section": "Data",
    "text": "Data\n\nanalysis_train &lt;- readRDS(\"data/analysis_train.rds\")\nanalysis_folds &lt;- readRDS(\"data/analysis_folds.rds\")"
  },
  {
    "objectID": "models/random-forest.html#model-recipe",
    "href": "models/random-forest.html#model-recipe",
    "title": "random-forest",
    "section": "Model Recipe",
    "text": "Model Recipe\n\nforest_recipe &lt;-\n  recipe(WeaponCarryingSchool ~ ., data = analysis_train) |&gt; \n  step_impute_mode(all_nominal_predictors()) |&gt; \n  step_impute_mean(all_numeric_predictors()) |&gt; \n  step_dummy(all_nominal_predictors())\n\nforest_recipe\n\n\n\n\nâ”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n\n\n\n\nâ”€â”€ Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 10\n\n\n\n\n\nâ”€â”€ Operations \n\n\nâ€¢ Mode imputation for: all_nominal_predictors()\n\n\nâ€¢ Mean imputation for: all_numeric_predictors()\n\n\nâ€¢ Dummy variables from: all_nominal_predictors()"
  },
  {
    "objectID": "models/random-forest.html#model-specification",
    "href": "models/random-forest.html#model-specification",
    "title": "random-forest",
    "section": "Model Specification",
    "text": "Model Specification\n\nforest_spec &lt;- \n  rand_forest(\n    mtry = tune(),\n    min_n = tune(),\n    trees = 100\n  ) |&gt; \n  set_mode(\"classification\") |&gt; \n  set_engine(\"ranger\", importance = \"permutation\")\n\nforest_spec\n\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 100\n  min_n = tune()\n\nEngine-Specific Arguments:\n  importance = permutation\n\nComputational engine: ranger"
  },
  {
    "objectID": "models/random-forest.html#model-workflow",
    "href": "models/random-forest.html#model-workflow",
    "title": "random-forest",
    "section": "Model Workflow",
    "text": "Model Workflow\n\nforest_workflow &lt;-\n  workflow() |&gt; \n  add_recipe(forest_recipe) |&gt; \n  add_model(forest_spec)\n\nforest_workflow\n\nâ•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nPreprocessor: Recipe\nModel: rand_forest()\n\nâ”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n3 Recipe Steps\n\nâ€¢ step_impute_mode()\nâ€¢ step_impute_mean()\nâ€¢ step_dummy()\n\nâ”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 100\n  min_n = tune()\n\nEngine-Specific Arguments:\n  importance = permutation\n\nComputational engine: ranger"
  },
  {
    "objectID": "models/random-forest.html#hyperparameter-tuning",
    "href": "models/random-forest.html#hyperparameter-tuning",
    "title": "random-forest",
    "section": "Hyperparameter Tuning",
    "text": "Hyperparameter Tuning\n\nforest_tune &lt;-\n  tune_grid(\n    forest_workflow,\n    resamples = analysis_folds,\n    grid = 11\n  )\n\nsaveRDS(forest_tune, \"model_outputs/forest_tune.rds\")\n\nforest_tune\n\n\n\n# Tuning results\n# 5-fold cross-validation \n# A tibble: 5 Ã— 4\n  splits               id    .metrics          .notes          \n  &lt;list&gt;               &lt;chr&gt; &lt;list&gt;            &lt;list&gt;          \n1 &lt;split [11756/2940]&gt; Fold1 &lt;tibble [33 Ã— 6]&gt; &lt;tibble [0 Ã— 3]&gt;\n2 &lt;split [11757/2939]&gt; Fold2 &lt;tibble [33 Ã— 6]&gt; &lt;tibble [0 Ã— 3]&gt;\n3 &lt;split [11757/2939]&gt; Fold3 &lt;tibble [33 Ã— 6]&gt; &lt;tibble [0 Ã— 3]&gt;\n4 &lt;split [11757/2939]&gt; Fold4 &lt;tibble [33 Ã— 6]&gt; &lt;tibble [0 Ã— 3]&gt;\n5 &lt;split [11757/2939]&gt; Fold5 &lt;tibble [33 Ã— 6]&gt; &lt;tibble [0 Ã— 3]&gt;"
  },
  {
    "objectID": "models/random-forest.html#collect-tuning-metrics",
    "href": "models/random-forest.html#collect-tuning-metrics",
    "title": "random-forest",
    "section": "Collect Tuning Metrics",
    "text": "Collect Tuning Metrics\n\ncollect_metrics(forest_tune)\n\n# A tibble: 33 Ã— 8\n    mtry min_n .metric     .estimator   mean     n std_err .config              \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1     1    17 accuracy    binary     0.957      5 0.00142 Preprocessor1_Model01\n 2     1    17 brier_class binary     0.0402     5 0.00125 Preprocessor1_Model01\n 3     1    17 roc_auc     binary     0.685      5 0.0100  Preprocessor1_Model01\n 4     1    32 accuracy    binary     0.957      5 0.00142 Preprocessor1_Model02\n 5     1    32 brier_class binary     0.0402     5 0.00125 Preprocessor1_Model02\n 6     1    32 roc_auc     binary     0.682      5 0.00940 Preprocessor1_Model02\n 7     2     5 accuracy    binary     0.957      5 0.00137 Preprocessor1_Model03\n 8     2     5 brier_class binary     0.0400     5 0.00124 Preprocessor1_Model03\n 9     2     5 roc_auc     binary     0.683      5 0.0100  Preprocessor1_Model03\n10     3    21 accuracy    binary     0.957      5 0.00140 Preprocessor1_Model04\n# â„¹ 23 more rows"
  },
  {
    "objectID": "models/random-forest.html#visualize-metrics",
    "href": "models/random-forest.html#visualize-metrics",
    "title": "random-forest",
    "section": "Visualize Metrics",
    "text": "Visualize Metrics\n\nautoplot(forest_tune)\n\n\n\n\n\n\n\n\n\nbest_forest &lt;- select_best(forest_tune, metric = \"roc_auc\")\n\nbest_forest\n\n# A tibble: 1 Ã— 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1     1    17 Preprocessor1_Model01"
  },
  {
    "objectID": "models/random-forest.html#finalize-model-workflow",
    "href": "models/random-forest.html#finalize-model-workflow",
    "title": "random-forest",
    "section": "Finalize Model Workflow",
    "text": "Finalize Model Workflow\n\nforest_final_workflow &lt;- finalize_workflow(forest_workflow, best_forest)\n\nforest_final_workflow\n\nâ•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nPreprocessor: Recipe\nModel: rand_forest()\n\nâ”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n3 Recipe Steps\n\nâ€¢ step_impute_mode()\nâ€¢ step_impute_mean()\nâ€¢ step_dummy()\n\nâ”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = 1\n  trees = 100\n  min_n = 17\n\nEngine-Specific Arguments:\n  importance = permutation\n\nComputational engine: ranger"
  },
  {
    "objectID": "models/random-forest.html#fit-the-model",
    "href": "models/random-forest.html#fit-the-model",
    "title": "random-forest",
    "section": "Fit the Model",
    "text": "Fit the Model\n\nforest_fit &lt;- fit(forest_final_workflow, analysis_train)\n\nsaveRDS(forest_fit, \"model_outputs/forest_fit.rds\")\n\nforest_fit\n\n\n\nâ•â• Workflow [trained] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nPreprocessor: Recipe\nModel: rand_forest()\n\nâ”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n3 Recipe Steps\n\nâ€¢ step_impute_mode()\nâ€¢ step_impute_mean()\nâ€¢ step_dummy()\n\nâ”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n$predictions\n                 0          1\n    [1,] 0.9608257 0.03917430\n    [2,] 0.9568533 0.04314667\n    [3,] 0.9651574 0.03484262\n    [4,] 0.9609997 0.03900031\n    [5,] 0.8917902 0.10820980\n    [6,] 0.9597347 0.04026534\n    [7,] 0.9686024 0.03139763\n    [8,] 0.9689044 0.03109564\n    [9,] 0.9456051 0.05439494\n   [10,] 0.9474776 0.05252242\n   [11,] 0.9529758 0.04702418\n   [12,] 0.9703308 0.02966922\n   [13,] 0.9440026 0.05599735\n   [14,] 0.9640737 0.03592628\n   [15,] 0.9307460 0.06925404\n   [16,] 0.9513482 0.04865182\n   [17,] 0.9378970 0.06210295\n   [18,] 0.9650748 0.03492525\n   [19,] 0.9615630 0.03843704\n   [20,] 0.9547721 0.04522794\n   [21,] 0.9679067 0.03209330\n   [22,] 0.9694196 0.03058044\n   [23,] 0.9306889 0.06931114\n   [24,] 0.9556499 0.04435009\n   [25,] 0.9497248 0.05027516\n   [26,] 0.9694189 0.03058112\n   [27,] 0.9694646 0.03053542\n   [28,] 0.9550579 0.04494215\n   [29,] 0.8869398 0.11306018\n   [30,] 0.9477009 0.05229912\n   [31,] 0.9497483 0.05025172\n   [32,] 0.9698491 0.03015094\n   [33,] 0.9693925 0.03060747\n   [34,] 0.9700790 0.02992103\n   [35,] 0.9703145 0.02968552\n   [36,] 0.9577626 0.04223742\n   [37,] 0.9706744 0.02932564\n   [38,] 0.9672956 0.03270444\n   [39,] 0.9573243 0.04267572\n   [40,] 0.9622386 0.03776138\n   [41,] 0.9649167 0.03508327\n   [42,] 0.9697737 0.03022634\n   [43,] 0.9541836 0.04581642\n   [44,] 0.9553624 0.04463762\n   [45,] 0.9683648 0.03163522\n   [46,] 0.9696704 0.03032959\n   [47,] 0.9687350 0.03126499\n   [48,] 0.9702012 0.02979883\n\n...\nand 30337 more lines."
  },
  {
    "objectID": "models/random-forest.html#make-model-predictions",
    "href": "models/random-forest.html#make-model-predictions",
    "title": "random-forest",
    "section": "Make Model Predictions",
    "text": "Make Model Predictions\n\nforest_pred &lt;- \n  augment(forest_fit, analysis_train) |&gt; \n  select(WeaponCarryingSchool, .pred_class, .pred_1, .pred_0)\n\nforest_pred\n\n# A tibble: 14,696 Ã— 4\n   WeaponCarryingSchool .pred_class .pred_1 .pred_0\n   &lt;fct&gt;                &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n 1 0                    0            0.0456   0.954\n 2 0                    0            0.0430   0.957\n 3 0                    0            0.0329   0.967\n 4 0                    0            0.0394   0.961\n 5 0                    0            0.109    0.891\n 6 0                    0            0.0456   0.954\n 7 0                    0            0.0308   0.969\n 8 0                    0            0.0308   0.969\n 9 0                    0            0.0512   0.949\n10 0                    0            0.0487   0.951\n# â„¹ 14,686 more rows"
  },
  {
    "objectID": "models/random-forest.html#roc-plot",
    "href": "models/random-forest.html#roc-plot",
    "title": "random-forest",
    "section": "ROC Plot",
    "text": "ROC Plot\n\nroc_forest &lt;- \n  forest_pred |&gt; \n  roc_curve(\n    truth = WeaponCarryingSchool,\n    .pred_1,\n    event_level = \"second\"\n  ) |&gt; \n  autoplot()\n\nsaveRDS(roc_forest, \"roc_graphs/forest.rds\")\n\nroc_forest\n\n\n\n\n\n\n\n\n\n\n\nforest_pred |&gt; \n  roc_auc(\n    truth = WeaponCarryingSchool,\n    .pred_1,\n    event_level = \"second\"\n  )\n\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.693"
  },
  {
    "objectID": "models/random-forest.html#resample-metrics",
    "href": "models/random-forest.html#resample-metrics",
    "title": "random-forest",
    "section": "Resample Metrics",
    "text": "Resample Metrics\n\nfit_resamples(forest_final_workflow, resamples = analysis_folds) |&gt; \n  collect_metrics()\n\n# A tibble: 3 Ã— 6\n  .metric     .estimator   mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.957      5 0.00142 Preprocessor1_Model1\n2 brier_class binary     0.0402     5 0.00126 Preprocessor1_Model1\n3 roc_auc     binary     0.686      5 0.0102  Preprocessor1_Model1"
  }
]